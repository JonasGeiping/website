
@article{borgnia_dp-instahide_2021,
  title = {{{DP}}-{{InstaHide}}: Provably {{Defusing Poisoning}} and {{Backdoor Attacks}} with {{Differentially Private Data Augmentations}}},
  shorttitle = {{{DP}}-{{InstaHide}}},
  author = {Borgnia, Eitan and Geiping, Jonas and Cherepanova, Valeriia and Fowl, Liam and Gupta, Arjun and Ghiasi, Amin and Huang, Furong and Goldblum, Micah and Goldstein, Tom},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.02079 [cs]},
  eprint = {2103.02079},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Data poisoning and backdoor attacks manipulate training data to induce security breaches in a victim model. These attacks can be provably deflected using differentially private (DP) training methods, although this comes with a sharp decrease in model performance. The InstaHide method has recently been proposed as an alternative to DP training that leverages supposed privacy properties of the mixup augmentation, although without rigorous guarantees. In this work, we show that strong data augmentations, such as mixup and random additive noise, nullify poison attacks while enduring only a small accuracy trade-off. To explain these finding, we propose a training method, DP-InstaHide, which combines the mixup regularizer with additive noise. A rigorous analysis of DP-InstaHide shows that mixup does indeed have privacy advantages, and that training with k-way mixup provably yields at least k times stronger DP guarantees than a naive DP mechanism. Because mixup (as opposed to noise) is beneficial to model performance, DP-InstaHide provides a mechanism for achieving stronger empirical performance against poisoning attacks than other known DP methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@inproceedings{borgnia_strong_2021,
  title = {Strong {{Data Augmentation Sanitizes Poisoning}} and {{Backdoor Attacks Without}} an {{Accuracy Tradeoff}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Borgnia, Eitan and Cherepanova, Valeriia and Fowl, Liam and Ghiasi, Amin and Geiping, Jonas and Goldblum, Micah and Goldstein, Tom and Gupta, Arjun},
  year = {2021},
  month = jun,
  pages = {3855--3859},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9414862},
  abstract = {Data poisoning and backdoor attacks manipulate victim models by maliciously modifying training data. In light of this growing threat, a recent survey of industry professionals revealed heightened fear in the private sector regarding data poisoning. Many previous defenses against poisoning either fail in the face of increasingly strong attacks, or they significantly degrade performance. However, we find that strong data augmentations, such as mixup and CutMix, can significantly diminish the threat of poisoning and backdoor attacks without trading off performance. We further verify the effectiveness of this simple defense against adaptive poisoning methods, and we compare to baselines including the popular differentially private SGD (DP-SGD) defense. In the context of backdoors, CutMix greatly mitigates the attack while simultaneously increasing validation accuracy by 9\%.},
  keywords = {Adversarial Attacks,Backdoor Attacks,Conferences,Data Augmentation,Data models,Data Poisoning,Differential Privacy,Industries,Machine learning,Signal processing,Toxicology,Training data}
}

@inproceedings{chiang_witchcraft_2020,
  title = {Witchcraft: Efficient {{PGD Attacks}} with {{Random Step Size}}},
  shorttitle = {Witchcraft},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chiang, Ping-Yeh and Geiping, Jonas and Goldblum, Micah and Goldstein, Tom and Ni, Renkun and Reich, Steven and Shafahi, Ali},
  year = {2020},
  month = may,
  pages = {3747--3751},
  issn = {2379-190X},
  doi = {10.1109/ICASSP40776.2020.9052930},
  abstract = {State-of-the-art adversarial attacks on neural networks use expensive iterative methods and numerous random restarts from different initial points. Iterative FGSM-based methods without restarts trade off performance for computational efficiency because they do not adequately explore the image space and are highly sensitive to the choice of step size. We propose a variant of Projected Gradient Descent (PGD) that uses a random step size to improve performance without resorting to expensive random restarts. Our method, Wide Iterative Stochastic crafting (WITCHcraft), achieves results superior to the classical PGD attack on the CIFAR-10 and MNIST data sets but without additional computational cost. This simple modification of PGD makes crafting attacks more economical, which is important in situations like adversarial training where attacks need to be crafted in real time.},
  keywords = {Adversarial,adversarial attacks,Attack,CIFAR,classical PGD attack,CNN,gradient methods,iterative FGSM-based methods,iterative methods,learning (artificial intelligence),neural nets,neural networks,PGD,PGD attacks,projected gradient descent,random step size,stochastic processes,wide iterative stochastic crafting,witchcraft},
  annotation = {ZSCC: 0000002}
}

@article{fowl_adversarial_2021,
  title = {Adversarial {{Examples Make Strong Poisons}}},
  author = {Fowl, Liam and Goldblum, Micah and Chiang, Ping-yeh and Geiping, Jonas and Czaja, Wojtek and Goldstein, Tom},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.10807 [cs]},
  eprint = {2106.10807},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The adversarial machine learning literature is largely partitioned into evasion attacks on testing data and poisoning attacks on training data. In this work, we show that adversarial examples, originally intended for attacking pre-trained models, are even more effective for data poisoning than recent methods designed specifically for poisoning. Our findings indicate that adversarial examples, when assigned the original label of their natural base image, cannot be used to train a classifier for natural images. Furthermore, when adversarial examples are assigned their adversarial class label, they are useful for training. This suggests that adversarial examples contain useful semantic content, just with the ``wrong'' labels (according to a network, but not a human). Our method, adversarial poisoning, is substantially more effective than existing poisoning methods for secure dataset release, and we release a poisoned version of ImageNet, ImageNet-P, to encourage research into the strength of this form of data obfuscation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@article{fowl_preventing_2021,
  title = {Preventing {{Unauthorized Use}} of {{Proprietary Data}}: Poisoning for {{Secure Dataset Release}}},
  shorttitle = {Preventing {{Unauthorized Use}} of {{Proprietary Data}}},
  author = {Fowl, Liam and Chiang, Ping-yeh and Goldblum, Micah and Geiping, Jonas and Bansal, Arpit and Czaja, Wojtek and Goldstein, Tom},
  year = {2021},
  month = feb,
  journal = {arXiv:2103.02683 [cs]},
  eprint = {2103.02683},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Large organizations such as social media companies continually release data, for example user images. At the same time, these organizations leverage their massive corpora of released data to train proprietary models that give them an edge over their competitors. These two behaviors can be in conflict as an organization wants to prevent competitors from using their own data to replicate the performance of their proprietary models. We solve this problem by developing a data poisoning method by which publicly released data can be minimally modified to prevent others from train-ing models on it. Moreover, our method can be used in an online fashion so that companies can protect their data in real time as they release it.We demonstrate the success of our approach onImageNet classification and on facial recognition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@article{gandikota_training_2021,
  title = {Training or {{Architecture}}? How to {{Incorporate Invariance}} in {{Neural Networks}}},
  shorttitle = {Training or {{Architecture}}?},
  author = {Gandikota, Kanchana Vaishnavi and Geiping, Jonas and L{\"a}hner, Zorah and Czapli{\'n}ski, Adam and Moeller, Michael},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.10044 [cs]},
  eprint = {2106.10044},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Many applications require the robustness, or ideally the invariance, of a neural network to certain transformations of input data. Most commonly, this requirement is addressed by either augmenting the training data, using adversarial training, or defining network architectures that include the desired invariance automatically. Unfortunately, the latter often relies on the ability to enlist all possible transformations, which make such approaches largely infeasible for infinite sets of transformations, such as arbitrary rotations or scaling. In this work, we propose a method for provably invariant network architectures with respect to group actions by choosing one element from a (possibly continuous) orbit based on a fixed criterion. In a nutshell, we intend to 'undo' any possible transformation before feeding the data into the actual network. We analyze properties of such approaches, extend them to equivariant networks, and demonstrate their advantages in terms of robustness as well as computational efficiency in several numerical examples. In particular, we investigate the robustness with respect to rotations of images (which can possibly hold up to discretization artifacts only) as well as the provable rotational and scaling invariance of 3D point cloud classification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {ZSCC: NoCitationData[s0]}
}

@phdthesis{geiping_comparison_2014,
  type = {Bachelor {{Thesis}}},
  title = {Comparison of {{Topology}}-Preserving {{Segmentation Methods}} and {{Application}} to {{Mitotic Cell Tracking}}},
  author = {Geiping, Jonas Alexander},
  year = {2014},
  month = sep,
  address = {{M\"unster}},
  school = {Westf\"alischen Wilhelms-Universit\"at M\"unster}
}

@article{geiping_composite_2018,
  title = {Composite {{Optimization}} by {{Nonconvex Majorization}}-{{Minimization}}},
  author = {Geiping, Jonas and Moeller, Michael},
  year = {2018},
  month = jan,
  journal = {SIAM Journal on Imaging Sciences},
  pages = {2494--2528},
  doi = {10.1137/18M1171989},
  abstract = {The minimization of a nonconvex composite function can model a variety of imaging tasks. A popular class of algorithms for solving such problems are majorization-minimization techniques which iteratively approximate the composite nonconvex function by a majorizing function that is easy to minimize. Most techniques, e.g., gradient descent, utilize convex majorizers in order to guarantee that the majorizer is easy to minimize. In our work we consider a natural class of nonconvex majorizers for these functions, and show that these majorizers are still sufficient for a globally convergent optimization scheme. Numerical results illustrate that by applying this scheme, one can often obtain superior local optima compared to previous majorization-minimization methods, when the nonconvex majorizers are solved to global optimality. Finally, we illustrate the behavior of our algorithm for depth superresolution from raw time-of-flight data.},
  keywords = {90C26; 90C06; 68U10; 32B20; 65K10; 47J06,Computer Science - Computer Vision and Pattern Recognition,Mathematics - Numerical Analysis,Mathematics - Optimization and Control}
}

@article{geiping_darts_2021,
  title = {{{DARTS}} for {{Inverse Problems}}: A {{Study}} on {{Hyperparameter Sensitivity}}},
  shorttitle = {{{DARTS}} for {{Inverse Problems}}},
  author = {Geiping, Jonas and Lukasik, Jovita and Keuper, Margret and Moeller, Michael},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.05647 [cs]},
  eprint = {2108.05647},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Differentiable architecture search (DARTS) is a widely researched tool for neural architecture search, due to its promising results for image classification. The main benefit of DARTS is the effectiveness achieved through the weight-sharing one-shot paradigm, which allows efficient architecture search. In this work, we investigate DARTS in a systematic case study of inverse problems, which allows us to analyze these potential benefits in a controlled manner. Although we demonstrate that the success of DARTS can be extended from image classification to reconstruction, our experiments yield three fundamental difficulties in the evaluation of DARTS-based methods: First, the results show a large variance in all test cases. Second, the final performance is highly dependent on the hyperparameters of the optimizer. And third, the performance of the weight-sharing architecture used during training does not reflect the final performance of the found architecture well. Thus, we conclude the necessity to 1) report the results of any DARTS-based methods from several runs along with its underlying performance statistics, 2) show the correlation of the training and final architecture performance, and 3) carefully consider if the computational efficiency of DARTS outweighs the costs of hyperparameter optimization and multiple runs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@inproceedings{geiping_fast_2020-1,
  title = {Fast {{Convex Relaxations}} Using {{Graph Discretizations}}},
  booktitle = {31st {{British Machine Vision Conference}} ({{BMVC}} 2020, {{Oral Presentation}})},
  author = {Geiping, Jonas and Gaede, Fjedor and Bauermeister, Hartmut and Moeller, Michael},
  year = {2020},
  month = sep,
  address = {{Virtual}},
  abstract = {Matching and partitioning problems are fundamentals of computer vision applications with examples in multilabel segmentation, stereo estimation and optical-flow computation. These tasks can be posed as non-convex energy minimization problems and solved near-globally optimal by recent convex lifting approaches. Yet, applying these techniques comes with a significant computational effort, reducing their feasibility in practical applications. We discuss spatial discretization of continuous partitioning problems into a graph structure, generalizing discretization onto a Cartesian grid. This setup allows us to faithfully work on super-pixel graphs constructed by SLIC or Cut-Pursuit, massively decreasing the computational effort for lifted partitioning problems compared to a Cartesian grid, while optimal energy values remain similar: The global matching is still solved near-globally optimal. We discuss this methodology in detail and show examples in multi-label segmentation by minimal partitions and stereo estimation, where we demonstrate that the proposed graph discretization can reduce runtime as well as memory consumption of convex relaxations of matching problems by up to a factor of 10.},
  copyright = {All rights reserved},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Mathematics - Optimization and Control}
}

@phdthesis{geiping_image_2016,
  type = {Master {{Thesis}}},
  title = {Image {{Analysis}} of {{Neural Tissue Development}}: Variational {{Methods}} for {{Segmentation}} and {{3D}}-{{Reconstruction}} from Large Pinhole Confocal Fluorescence Microscopy},
  author = {Geiping, Jonas Alexander},
  year = {2016},
  month = sep,
  address = {{M\"unster}},
  abstract = {Three-dimensional time series data from confocal fluorescence microscopes is a valuable tool in biological research, but the data is distorted by Poisson noise and defocus blur of varying axial extent. We seek to obtain structural information about the develop- ment of neural tissue from these images and define a segmentation by an appropriate thresholding of reconstructed data. We model the data degradation and develop a reconstruction formulation based on variational methods. Due to imprecise knowledge of the blur kernel we extend local sparsity regularization to a local patch and use this prior as additional regularization. We show favorable analytical properties for this approach, implement the resulting algorithm with a primal-dual optimization scheme and test on artificial and real data.},
  school = {Westf\"alischen Wilhelms-Universit\"at M\"unster},
  annotation = {00000}
}

@inproceedings{geiping_inverting_2020-1,
  title = {Inverting {{Gradients}} - {{How}} Easy Is It to Break Privacy in Federated Learning?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Geiping, Jonas and Bauermeister, Hartmut and Dr{\"o}ge, Hannah and Moeller, Michael},
  year = {2020},
  month = dec,
  volume = {33},
  langid = {english},
  annotation = {ZSCC: 0000016}
}

@phdthesis{geiping_modern_2021,
  type = {Doctoral {{Thesis}}},
  title = {Modern Optimization Techniques in Computer Vision},
  author = {Geiping, Jonas},
  year = {2021},
  address = {{10.25819/ubsi/9908}},
  abstract = {This thesis presents research into multiple optimization topics in computer vision with a conceptual focus on composite optimization problems such as bilevel optimization. The optimal graph-based discretization of variational problems in minimal partitions, the theoretical analysis of nonconvex composite optimization by nonconvex majorizers, the bilevel problem of learning energy models by nonconvex majorizers, and the machine learning security applications of bilevel optimization in privacy analysis of federated learning and dataset poisoning of general image classification are featured in this cumulative work.},
  langid = {english},
  school = {University of Siegen},
  annotation = {Accepted: 2021-05-10T11:11:26Z}
}

@inproceedings{geiping_multiframe_2018,
  title = {Multiframe {{Motion Coupling}} for {{Video Super Resolution}}},
  booktitle = {Energy {{Minimization Methods}} in {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Geiping, Jonas and Dirks, Hendrik and Cremers, Daniel and Moeller, Michael},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {123--138},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-78199-0 _ 9},
  abstract = {The idea of video super resolution is to use different view points of a single scene to enhance the overall resolution and quality. Classical energy minimization approaches first establish a correspondence of the current frame to all its neighbors in some radius and then use this temporal information for enhancement. In this paper, we propose the first variational super resolution approach that computes several super resolved frames in one batch optimization procedure by incorporating motion information between the high-resolution image frames themselves. As a consequence, the number of motion estimation problems grows linearly in the number of frames, opposed to a quadratic growth of classical methods and temporal consistency is enforced naturally.We use infimal convolution regularization as well as an automatic parameter balancing scheme to automatically determine the reliability of the motion information and reweight the regularization locally. We demonstrate that our approach yields state-of-the-art results and even is competitive with machine learning approaches.},
  isbn = {978-3-319-78199-0},
  langid = {english}
}

@inproceedings{geiping_parametric_2019-2,
  title = {Parametric {{Majorization}} for {{Data}}-{{Driven Energy Minimization Methods}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Geiping, Jonas and Moeller, Michael},
  year = {2019},
  pages = {10262--10273},
  abstract = {Energy minimization methods are a classical tool in a multitude of computer vision applications. While they are interpretable and well-studied, their regularity assumptions are difficult to design by hand. Deep learning techniques on the other hand are purely data-driven, often provide excellent results, but are very difficult to constrain to predefined physical or safety-critical models. A possible combination between the two approaches is to design a parametric en- ergy and train the free parameters in such a way that minimizers of the energy correspond to desired solution on a set of training examples. Unfortunately, such formulations typically lead to bi-level optimization problems, on which common optimization algorithms are difficult to scale to modern requirements in data processing and efficiency. In this work, we present a new strategy to optimize these bi-level problems. We investigate surrogate single-level problems that majorize the target problems and can be implemented with existing tools, leading to efficient algorithms without collapse of the energy function. This framework of strategies enables new avenues to the training of parameterized energy minimization models from large data.},
  copyright = {All rights reserved}
}

@article{geiping_stochastic_2021,
  title = {Stochastic {{Training}} Is {{Not Necessary}} for {{Generalization}}},
  author = {Geiping, Jonas and Goldblum, Micah and Pope, Phillip E. and Moeller, Michael and Goldstein, Tom},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.14119 null},
  eprint = {2109.14119},
  eprinttype = {arxiv},
  abstract = {It is widely believed that the implicit regularization of stochastic gradient descent (SGD) is fundamental to the impressive generalization behavior we observe in neural networks. In this work, we demonstrate that non-stochastic full-batch training can achieve strong performance on CIFAR-10 that is on-par with SGD, using modern architectures in settings with and without data augmentation. To this end, we utilize modified hyperparameters and show that the implicit regularization of SGD can be completely replaced with explicit regularization. This strongly suggests that theories that rely heavily on properties of stochastic sampling to explain generalization are incomplete, as strong generalization behavior is still observed in the absence of stochastic sampling. Fundamentally, deep learning can succeed without stochasticity. Our observations further indicate that the perceived difficulty of full-batch training is largely the result of its optimization properties and the disproportionate time and effort spent by the ML community tuning optimizers and hyperparameters for small-batch training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control}
}

@article{geiping_what_2021,
  title = {What {{Doesn}}'t {{Kill You Makes You Robust}}(Er): Adversarial {{Training}} against {{Poisons}} and {{Backdoors}}},
  shorttitle = {What {{Doesn}}'t {{Kill You Makes You Robust}}(Er)},
  author = {Geiping, Jonas and Fowl, Liam and Somepalli, Gowthami and Goldblum, Micah and Moeller, Michael and Goldstein, Tom},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.13624 [cs]},
  eprint = {2102.13624},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Data poisoning is a threat model in which a malicious actor tampers with training data to manipulate outcomes at inference time. A variety of defenses against this threat model have been proposed, but each suffers from at least one of the following flaws: they are easily overcome by adaptive attacks, they severely reduce testing performance, or they cannot generalize to diverse data poisoning threat models. Adversarial training, and its variants, is currently considered the only empirically strong defense against (inference-time) adversarial attacks. In this work, we extend the adversarial training framework to instead defend against (training-time) poisoning and backdoor attacks. Our method desensitizes networks to the effects of poisoning by creating poisons during training and injecting them into training batches. We show that this defense withstands adaptive attacks, generalizes to diverse threat models, and incurs a better performance trade-off than previous defenses.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@inproceedings{geiping_witches_2021,
  title = {Witches' {{Brew}}: Industrial {{Scale Data Poisoning}} via {{Gradient Matching}}},
  shorttitle = {Witches' {{Brew}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Geiping, Jonas and Fowl, Liam H. and Huang, W. Ronny and Czaja, Wojciech and Taylor, Gavin and Moeller, Michael and Goldstein, Tom},
  year = {2021},
  month = apr,
  abstract = {Data Poisoning attacks modify training data to maliciously control a model trained on such data. Previous poisoning attacks against deep neural networks have been limited in scope and success...},
  langid = {english}
}

@inproceedings{goldblum_truth_2020,
  title = {Truth or Backpropaganda? An Empirical Investigation of Deep Learning Theory},
  shorttitle = {Truth or Backpropaganda?},
  booktitle = {Eighth {{International Conference}} on {{Learning Representations}} ({{ICLR}} 2020, {{Oral Presentation}})},
  author = {Goldblum, Micah and Geiping, Jonas and Schwarzschild, Avi and Moeller, Michael and Goldstein, Tom},
  year = {2020},
  month = apr,
  abstract = {We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike. In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role; (4) find that rank does not correlate with generalization or robustness in a practical setting.},
  copyright = {All rights reserved},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]}
}

@inproceedings{gorlitz_piecewise_2019-1,
  title = {Piecewise {{Rigid Scene Flow}} with {{Implicit Motion Segmentation}}},
  booktitle = {2019 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {G{\"o}rlitz, Andreas and Geiping, Jonas and Kolb, Andreas},
  year = {2019},
  month = nov,
  pages = {1758--1765},
  issn = {2153-0866},
  doi = {10.1109/IROS40897.2019.8968018},
  abstract = {In this paper, we introduce a novel variational approach to estimate the scene flow from RGB-D images. We regularize the ill-conditioned problem of scene flow estimation in a unified framework by enforcing piecewise rigid motion through decomposition into rotational and translational motion parts. Our model crucially regularizes these components by an L0 ``norm'', thereby facilitating implicit motion segmentation in a joint energy minimization problem. Yet, we also show that this energy can be efficiently minimized by a proximal primal-dual algorithm. By implementing this approximate L0 rigid motion regularization, our scene flow estimation approach implicitly segments the observed scene of into regions of nearly constant rigid motion. We evaluate our joint scene flow and segmentation estimation approach on a variety of test scenarios, with and without ground truth data, and demonstrate that we outperform current scene flow techniques.},
  keywords = {constant rigid motion,ill-conditioned problem,image colour analysis,image segmentation,image sequences,implicit motion segmentation estimation approach,joint energy minimization problem,L0 rigid motion regularization,minimisation,motion estimation,observed scene,piecewise rigid motion,piecewise rigid scene flow techniques,proximal primal-dual algorithm,RGB-D images,rotational motion parts,scene flow estimation approach,translational motion parts,variational approach}
}

@inproceedings{huang_metapoison_2020-1,
  title = {{{MetaPoison}}: Practical {{General}}-Purpose {{Clean}}-Label {{Data Poisoning}}},
  shorttitle = {{{MetaPoison}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Huang, W. Ronny and Geiping, Jonas and Fowl, Liam and Taylor, Gavin and Goldstein, Tom},
  year = {2020},
  month = dec,
  volume = {33},
  address = {{Vancouver, Canada}},
  langid = {english},
  annotation = {ZSCC: 0000007}
}


