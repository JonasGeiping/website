@inproceedings{chiang_witchcraft_2020,
 abstract = {State-of-the-art adversarial attacks on neural networks use expensive iterative methods and numerous random restarts from different initial points. Iterative FGSM-based methods without restarts trade off performance for computational efficiency because they do not adequately explore the image space and are highly sensitive to the choice of step size. We propose a variant of Projected Gradient Descent (PGD) that uses a random step size to improve performance without resorting to expensive random restarts. Our method, Wide Iterative Stochastic crafting (WITCHcraft), achieves results superior to the classical PGD attack on the CIFAR-10 and MNIST data sets but without additional computational cost. This simple modification of PGD makes crafting attacks more economical, which is important in situations like adversarial training where attacks need to be crafted in real time.},
 annotation = {ZSCC: 0000002},
 author = {Chiang, Ping-Yeh and Geiping, Jonas and Goldblum, Micah and Goldstein, Tom and Ni, Renkun and Reich, Steven and Shafahi, Ali},
 booktitle = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
 doi = {10.1109/ICASSP40776.2020.9052930},
 issn = {2379-190X},
 keywords = {Adversarial,adversarial attacks,Attack,CIFAR,classical PGD attack,CNN,gradient methods,iterative FGSM-based methods,iterative methods,learning (artificial intelligence),neural nets,neural networks,PGD,PGD attacks,projected gradient descent,random step size,stochastic processes,wide iterative stochastic crafting,witchcraft},
 month = {May},
 pages = {3747--3751},
 shorttitle = {Witchcraft},
 title = {Witchcraft: Efficient PGD Attacks with Random Step Size},
 year = {2020}
}

