@article{geiping_what_2021,
 abstract = {Data poisoning is a threat model in which a malicious actor tampers with training data to manipulate outcomes at inference time. A variety of defenses against this threat model have been proposed, but each suffers from at least one of the following flaws: they are easily overcome by adaptive attacks, they severely reduce testing performance, or they cannot generalize to diverse data poisoning threat models. Adversarial training, and its variants, is currently considered the only empirically strong defense against (inference-time) adversarial attacks. In this work, we extend the adversarial training framework to instead defend against (training-time) poisoning and backdoor attacks. Our method desensitizes networks to the effects of poisoning by creating poisons during training and injecting them into training batches. We show that this defense withstands adaptive attacks, generalizes to diverse threat models, and incurs a better performance trade-off than previous defenses.},
 archiveprefix = {arXiv},
 author = {Geiping, Jonas and Fowl, Liam and Somepalli, Gowthami and Goldblum, Micah and Moeller, Michael and Goldstein, Tom},
 copyright = {All rights reserved},
 eprint = {2102.13624},
 eprinttype = {arxiv},
 journal = {arXiv:2102.13624 [cs]},
 keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
 month = {February},
 primaryclass = {cs},
 shorttitle = {What Doesn't Kill You Makes You Robust(Er)},
 title = {What Doesn't Kill You Makes You Robust(Er): Adversarial Training against Poisons and Backdoors},
 year = {2021}
}

