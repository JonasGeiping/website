@article{goldblum_truth_2019,
 abstract = {We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike. We study the prevalence of local minima in loss landscapes, whether small-norm parameter vectors generalize better (and whether this explains the advantages of weight decay), whether wide-network theories (like the neural tangent kernel) describe the behaviors of classifiers, and whether the rank of weight matrices can be linked to generalization and robustness in real-world networks.},
 archiveprefix = {arXiv},
 author = {Goldblum, Micah and Geiping, Jonas and Schwarzschild, Avi and Moeller, Michael and Goldstein, Tom},
 eprint = {1910.00359},
 eprinttype = {arxiv},
 journal = {ArXiv191000359 Cs Math Stat},
 keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
 month = {October},
 primaryclass = {cs, math, stat},
 shorttitle = {Truth or Backpropaganda?},
 title = {Truth or Backpropaganda? An Empirical Investigation of Deep Learning Theory},
 year = {2019}
}

