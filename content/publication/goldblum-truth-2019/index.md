---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: Truth or Backpropaganda? An Empirical Investigation of Deep Learning Theory
subtitle: ''
summary: ''
authors:
- Micah Goldblum
- Jonas Geiping
- Avi Schwarzschild
- Michael Moeller
- Tom Goldstein
tags:
- '"Computer Science - Machine Learning"'
- '"Mathematics - Optimization and Control"'
- '"Statistics - Machine Learning"'
categories: []
date: '2019-10-01'
lastmod: 2020-09-08T20:33:30+02:00
featured: true
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2020-09-08T18:33:30.263461Z'
publication_types:
- 2
abstract: We empirically evaluate common assumptions about neural networks that are
  widely held by practitioners and theorists alike. We study the prevalence of local
  minima in loss landscapes, whether small-norm parameter vectors generalize better
  (and whether this explains the advantages of weight decay), whether wide-network
  theories (like the neural tangent kernel) describe the behaviors of classifiers,
  and whether the rank of weight matrices can be linked to generalization and robustness
  in real-world networks.
publication: '*ArXiv191000359 Cs Math Stat*'
---
