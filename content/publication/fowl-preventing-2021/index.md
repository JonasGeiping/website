---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: 'Preventing Unauthorized Use of Proprietary Data: Poisoning for Secure Dataset
  Release'
subtitle: ''
summary: ''
authors:
- Liam Fowl
- Ping-yeh Chiang
- Micah Goldblum
- Jonas Geiping
- Arpit Bansal
- Wojtek Czaja
- Tom Goldstein
tags:
- Computer Science - Cryptography and Security
- Computer Science - Machine Learning
categories: []
date: '2021-02-01'
lastmod: 2022-03-07T13:07:32-05:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2022-03-07T18:07:32.660334Z'
publication_types:
- '2'
abstract: Large organizations such as social media companies continually release data,
  for example user images. At the same time, these organizations leverage their massive
  corpora of released data to train proprietary models that give them an edge over
  their competitors. These two behaviors can be in conflict as an organization wants
  to prevent competitors from using their own data to replicate the performance of
  their proprietary models. We solve this problem by developing a data poisoning method
  by which publicly released data can be minimally modified to prevent others from
  train-ing models on it. Moreover, our method can be used in an online fashion so
  that companies can protect their data in real time as they release it.We demonstrate
  the success of our approach onImageNet classification and on facial recognition.
publication: '*arXiv:2103.02683 [cs]*'
links:
- name: arXiv
  url: https://arxiv.org/abs/2103.02683
- name: URL
  url: http://arxiv.org/abs/2103.02683
---
